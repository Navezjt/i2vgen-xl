{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/camenduru/I2VGen-XL-colab/blob/main/I2VGen_XL_Vid2Vid_colab.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "!apt -y install -qq aria2\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/resolve/main/open_clip_pytorch_model.bin -d /root/.cache/modelscope/hub/damo/Video-to-Video -o open_clip_pytorch_model.bin\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/resolve/main/non_ema_0035000.pth -d /root/.cache/modelscope/hub/damo/Video-to-Video -o non_ema_0035000.pth\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/resolve/main/v2-1_512-ema-pruned.ckpt -d /root/.cache/modelscope/hub/damo/Video-to-Video -o v2-1_512-ema-pruned.ckpt\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/raw/main/configuration.json -d /root/.cache/modelscope/hub/damo/Video-to-Video -o configuration.json\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/resolve/main/.mdl -d /root/.cache/modelscope/hub/damo/Video-to-Video -o .mdl\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/resolve/main/.msc -d /root/.cache/modelscope/hub/damo/Video-to-Video -o .msc\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/damo-video-to-video/resolve/main/assets/images/Fig_1.png -d /root/.cache/modelscope/hub/damo/Video-to-Video -o assets/images/Fig_1.png\n",
        "\n",
        "!pip install -q xformers==0.0.20 triton==2.0.0 gradio torchsde open_clip_torch einops rotary-embedding-torch fairscale \n",
        "!pip install -q pytorch-lightning torchmetrics -U\n",
        "!pip install https://github.com/camenduru/I2VGen-XL-colab/releases/download/v1.0/modelscope-1.9.1-py3-none-any.whl\n",
        "\n",
        "# https://huggingface.co/spaces/damo-vilab/MS-Vid2Vid-XL-demo/blob/main/app.py modified\n",
        "import tempfile\n",
        "import cv2\n",
        "import gradio as gr\n",
        "from modelscope.outputs import OutputKeys\n",
        "from modelscope.pipelines import pipeline\n",
        "\n",
        "pipe = pipeline(task=\"video-to-video\", model='damo/Video-to-Video', model_revision='v1.1.0', device='cuda:0')\n",
        "\n",
        "def check_input_video(video_path: str) -> None:\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    cap.release()\n",
        "    if n_frames != 32 or width != 448 or height != 256:\n",
        "        raise gr.Error(\n",
        "            f\"Input video must be 32 frames of size 448x256. Your video is {n_frames} frames of size {width}x{height}.\"\n",
        "        )\n",
        "\n",
        "def video_to_video(video_path: str, text: str) -> str:\n",
        "    check_input_video(video_path)\n",
        "    p_input = {\"video_path\": video_path, \"text\": text}\n",
        "    output_file = tempfile.NamedTemporaryFile(suffix=\".mp4\", delete=False)\n",
        "    pipe(p_input, output_video=output_file.name)[OutputKeys.OUTPUT_VIDEO]\n",
        "    return output_file.name\n",
        "\n",
        "with gr.Blocks(css=\"style.css\") as demo:\n",
        "    with gr.Group():\n",
        "        input_video = gr.Video(label=\"Input video\", type=\"filepath\")\n",
        "        text_description = gr.Textbox(label=\"Text description\")\n",
        "        run_button = gr.Button()\n",
        "        output_video = gr.Video(label=\"Output video\")\n",
        "    text_description.submit(\n",
        "        fn=check_input_video,\n",
        "        inputs=input_video,\n",
        "        queue=False,\n",
        "        api_name=None,\n",
        "    ).success(\n",
        "        fn=video_to_video,\n",
        "        inputs=[input_video, text_description],\n",
        "        outputs=output_video,\n",
        "        api_name=None,\n",
        "    )\n",
        "    run_button.click(\n",
        "        fn=check_input_video,\n",
        "        inputs=input_video,\n",
        "        queue=False,\n",
        "        api_name=None,\n",
        "    ).success(\n",
        "        fn=video_to_video,\n",
        "        inputs=[input_video, text_description],\n",
        "        outputs=output_video,\n",
        "        api_name=\"run\",\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demo.queue(max_size=10).launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
